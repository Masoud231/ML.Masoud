import streamlit as st
import numpy as np
import joblib
import os
import cv2
import mediapipe as mp

st.set_page_config(page_title="Eye State Detection", layout="wide")

st.title("ğŸ‘ï¸ Eye State Detection System")
st.write("Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙˆØ¶Ø¹ÛŒØª Ú†Ø´Ù… Ø¨Ø§ Ø¯Ùˆ Ø±ÙˆØ´: EEG Ùˆ Ø¯ÙˆØ±Ø¨ÛŒÙ†")

tabs = st.tabs(["ğŸ”µ EEG Prediction", "ğŸŸ¢ Camera Eye Detection"])

# ============================================================
# TAB 1 â€” EEG MODEL
# ============================================================
with tabs[0]:
    st.header("Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ÙˆØ¶Ø¹ÛŒØª Ú†Ø´Ù… Ø¨Ø§ EEG")

    base = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(base, "eye_state_model.pkl")
    scaler_path = os.path.join(base, "scaler.pkl")

    if not os.path.exists(model_path) or not os.path.exists(scaler_path):
        st.error("âŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ EEG Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯Ù†Ø¯. Ù„Ø·ÙØ§Ù‹ eye_state_model.pkl Ùˆ scaler.pkl Ø±Ø§ Ø¯Ø± Ø±ÛŒÙ¾Ùˆ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
        st.stop()

    model = joblib.load(model_path)
    scaler = joblib.load(scaler_path)

    st.subheader("ÙˆØ±ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (14 Ù…Ù‚Ø¯Ø§Ø± EEG)")

    if st.button("ğŸ”„ ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡ EEG ØªØµØ§Ø¯ÙÛŒ"):
        random_sample = np.random.normal(0, 1, 14)
        for i in range(14):
            st.session_state[f"f{i}"] = float(random_sample[i])

    features = []
    for i in range(14):
        val = st.number_input(
            f"Feature {i+1}",
            value=st.session_state.get(f"f{i}", 0.0),
            format="%.4f"
        )
        features.append(val)

    if st.button("ğŸ” Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ EEG"):
        sample = np.array([features])
        sample_scaled = scaler.transform(sample)
        prediction = model.predict(sample_scaled)[0]

        if prediction == 0:
            st.error("ğŸ‘ï¸â€ğŸ—¨ï¸ Ù†ØªÛŒØ¬Ù‡: Ú†Ø´Ù… Ø¨Ø³ØªÙ‡")
        else:
            st.success("ğŸ‘ï¸ Ù†ØªÛŒØ¬Ù‡: Ú†Ø´Ù… Ø¨Ø§Ø²")


# ============================================================
# TAB 2 â€” CAMERA DETECTION
# ============================================================
with tabs[1]:
    st.header("ØªØ´Ø®ÛŒØµ Ø¨Ø§Ø²/Ø¨Ø³ØªÙ‡ Ø¨ÙˆØ¯Ù† Ú†Ø´Ù… Ø¨Ø§ Ø¯ÙˆØ±Ø¨ÛŒÙ†")

    run = st.checkbox("ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ø¯ÙˆØ±Ø¨ÛŒÙ†")

    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(
        max_num_faces=1,
        refine_landmarks=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )

    LEFT_EYE = [33, 160, 158, 133, 153, 144]
    RIGHT_EYE = [362, 385, 387, 263, 373, 380]

    def eye_aspect_ratio(landmarks, eye_indices):
        pts = np.array([(landmarks[i].x, landmarks[i].y) for i in eye_indices])
        A = np.linalg.norm(pts[1] - pts[5])
        B = np.linalg.norm(pts[2] - pts[4])
        C = np.linalg.norm(pts[0] - pts[3])
        return (A + B) / (2.0 * C)

    FRAME_WINDOW = st.image([])

    cap = cv2.VideoCapture(0)

    while run:
        ret, frame = cap.read()
        if not ret:
            break

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(rgb)

        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark

            left_ear = eye_aspect_ratio(landmarks, LEFT_EYE)
            right_ear = eye_aspect_ratio(landmarks, RIGHT_EYE)
            ear = (left_ear + right_ear) / 2

            if ear < 0.25:
                status = "Ú†Ø´Ù… Ø¨Ø³ØªÙ‡"
                color = (0, 0, 255)
            else:
                status = "Ú†Ø´Ù… Ø¨Ø§Ø²"
                color = (0, 255, 0)

            cv2.putText(frame, status, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)

        FRAME_WINDOW.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    cap.release()
